{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_probabilidad_em.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgarciasanes/metodosdeagrupamiento_nosupervisado/blob/master/05_probabilidad_em.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-NVHi_QCWM88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center><h1> Aplicación de los Métodos de Agrupamiento</h1></center>\n",
        "<center><h1> Aprendizaje No Supervisado</h1></center>\n",
        "<center><h1> Agrupamiento basado en probabilidades : EM</h1></center>\n",
        "<br>\n",
        "\n",
        "### - Alexandro López González\n",
        "### - Leonardo Pacheco Garduño\n",
        "### - Manuel Garcia Sanes\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "En este notebook, realizaremos los siguiente:\n",
        "\n",
        "- Aplicación del algoritmo EM  a los dos datasets seleccionados. \n",
        "\n",
        "- Evaluación de los resultados utilizando métricas intrínsecas o extrínsecas según sea el caso\n",
        "\n",
        "- Análisis del resultado\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "metadata": {
        "id": "rwhbOe26WM9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as it\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from scipy.stats import multivariate_normal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjpdx1IyWM9L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Cargamos el dataset con el que vamos a trabajar:\n"
      ]
    },
    {
      "metadata": {
        "id": "xuO-iutXWM9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "#data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_dos_guassianas.csv'\n",
        "data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_cuatro_diferente_medida.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "print('El dataset cargado tiene',Dy.size,'instancias.')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRWT7LWCWM9W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "El algoritmo EM tiene un único parámetro: el número de clústeres (K). Una vez fijado este valor, el primer paso consiste en inicializar el modelo. Se eligen unos centros iniciales de manera aleatoria, unas matrices de covarianzas fijas y unos pesos iniciales para las diferentes componentes. Sin más información, lo más normal sería asignar a todas las componentes el mismo peso.\n"
      ]
    },
    {
      "metadata": {
        "id": "anQnllm2WM9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################## INICIALIZACION ########################\n",
        "# Elegimos un número de clústeres a buscar\n",
        "K = 4\n",
        "\n",
        "# Asignar unas matrices de covarianzas iniciales\n",
        "sigmas = []\n",
        "for k in np.arange(K):\n",
        "    sigmas.append( np.diag( 0.1 * np.ones( Dx.shape[1] ) ) )\n",
        "\n",
        "x, y = np.mgrid[(np.min(Dx[:,0])-0.1):(np.max(Dx[:,0])+0.1):.01, \n",
        "                (np.min(Dx[:,1])-0.1):(np.max(Dx[:,1])+0.1):.01]\n",
        "pos = np.empty(x.shape + (2,))\n",
        "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
        "\n",
        "# Inicialmente consideramos que todas las componentes tienen la misma probabilidad\n",
        "PIs = np.ones(K)/K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__nwayXNWM9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_sample_float(n, mi, ma):\n",
        "    return (ma - mi) * np.random.random_sample(n) + mi\n",
        "\n",
        "# Elegir unos centros (uno para cada componente) de manera aleatoria\n",
        "cDx = np.zeros(K*Dx.shape[1])\n",
        "cDx.shape = (K,Dx.shape[1])\n",
        "\n",
        "for d in np.arange(Dx.shape[1]):\n",
        "    cDx[:,d] = random_sample_float(K, np.min(Dx[:,d]), np.max(Dx[:,d]))\n",
        "\n",
        "print('Los centros iniciales elegidos aleatoriamente son:')\n",
        "print(cDx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Raxk_6yJWM9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mostramos las componentes iniciales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "for k in np.arange(K):\n",
        "    rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
        "    ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
        "    ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
        "\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4IIQJ9-WM9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Una vez inicializado, el algoritmo EM ejecuta un bucle donde se repiten los pasos E y M hasta que se alcanza la convergencia. \n",
        "\n",
        "En el paso E se (re)calcula la probabilidad de que cada ejemplo pertenezca a cada una de las componentes (los valores $z_{ik}$). \n",
        "\n",
        "En el paso M se (re)calculan los parámetros del modelo: los centros de las distribuciones normales (uno por componente, $\\mu_k$), las matrices de covarianzas de las normales (una por componente, $\\Sigma_k$) y los coeficientes de importancia de las diferentes componentes ($\\{\\pi_k\\}_{k=1}^K$, con $\\sum_k \\pi_k=1$).\n",
        "\n",
        "El algoritmo alcanza la convergencia cuando los parámetros no cambian entre dos iteraciones consecutivas.\n"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "rxVbKUuDWM9v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preparamos el vector donde guardamos la asignación probabilística \n",
        "# de cada elemento a un clúster (z_ik)\n",
        "Dy_probs = np.zeros((Dx.shape[0], K))\n",
        "\n",
        "# Flag de convergencia\n",
        "iterando = True\n",
        "# Dibujar los plots intermedios?\n",
        "dibujar = True\n",
        "# Si dibujas, sólo uno de cada 'frec_dibujo'\n",
        "frec_dibujo = 10\n",
        "\n",
        "it = 0\n",
        "\n",
        "while iterando:\n",
        "\n",
        "    # Vector auxiliar para guardar los centros de la iteración pasada\n",
        "    # necesarios para identificar la convergencia\n",
        "    cDx_ant = cDx.copy()\n",
        "    \n",
        "    # PASO E:\n",
        "    # Calcular la asignacion a las componentes (z_ik)\n",
        "    for k in np.arange(K):\n",
        "        Dy_probs[:,k] = PIs[k] * multivariate_normal.pdf(Dx, mean=cDx[k,:], cov=sigmas[k])\n",
        "    Dy_probs = Dy_probs/np.sum(Dy_probs,axis=1)[:,None]\n",
        "    \n",
        "    # PASO M:\n",
        "    # - a: Calcular los nuevos centros de las K componentes\n",
        "    for k in range(K):\n",
        "        cDx[k,:] = np.sum(Dy_probs[:,k,None]*Dx,axis=0)/np.sum(Dy_probs[:,k])\n",
        "\n",
        "    # - b: Calcular la matriz de covarianza (sigma) de las K componentes\n",
        "    for k in range(K):\n",
        "        sigmas[k] = np.sum([Dy_probs[i,k]* np.dot(Dx[i,:,None]-cDx[k,:,None],\n",
        "                                                  (Dx[i,:,None]-cDx[k,:,None]).transpose())\n",
        "                            for i in np.arange(Dx.shape[0])], axis=0) / np.sum(Dy_probs[:,k])\n",
        "\n",
        "    # - c: Calcular los coeficientes de importancia de las diferentes componentes\n",
        "    PIs = np.sum(Dy_probs,axis=0)/Dx.shape[0]\n",
        "\n",
        "    if dibujar and (it % frec_dibujo) == 0:\n",
        "        # Dibujar el plot con el resultado actual\n",
        "        fig, ax = plt.subplots(figsize=(10,5))\n",
        "        for k in np.arange(K):\n",
        "            rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
        "            ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
        "            ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
        "        ax.scatter(Dx[:,0],Dx[:,1])\n",
        "        ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')\n",
        "\n",
        "    it += 1\n",
        "    if np.allclose(cDx, cDx_ant):\n",
        "        break\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4ot_JzEWM93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "El resultado final del algoritmo es una matriz de probabilidades que asigna cada elemento a un clúster (componente) con cierta probabilidad. Si quisiésemos obtener una asignación determinista de cada caso a un único clúster, tomaríamos la componente que mayor probabilidad asigna a cada caso como su clúster:\n"
      ]
    },
    {
      "metadata": {
        "id": "iT_D7UOZWM97",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Dyp = np.argmax(Dy_probs,axis=1)\n",
        "\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "for k in np.arange(K):\n",
        "    rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
        "    ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
        "    ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp)\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pdJlPdsUWM-A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Como en anteriores ocasiones, podemos estudiar la bondad del agrupamiento ya que se conoce la realidad:\n"
      ]
    },
    {
      "metadata": {
        "id": "JrAwsAleWM-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matriz_confusion(cat_real, cat_pred):\n",
        "    cats = np.unique(cat_real)\n",
        "    clusts = np.unique(cat_pred)\n",
        "    mat = np.array([[np.sum(np.logical_and(cat_real==cats[i], cat_pred==clusts[j])) \n",
        "                     for j in np.arange(clusts.size)] \n",
        "                    for i in np.arange(cats.size)])\n",
        "    return(mat)\n",
        "\n",
        "def medida_error(mat):\n",
        "    assign = np.sum([np.max(mat[l,:]) for l in np.arange(mat.shape[0])])\n",
        "    return 1 - assign / float(np.sum(mat))\n",
        "\n",
        "def medida_pureza(mat):\n",
        "    totales = np.sum(mat,0)/float(np.sum(mat))\n",
        "    return np.sum([totales[k] * np.max(mat[:,k]/float(np.sum(mat[:,k]))) for k in np.arange(mat.shape[1])])\n",
        "\n",
        "def medida_precision(mat, l, k):\n",
        "    return mat[l,k]/float(np.sum(mat[:,k]))\n",
        "\n",
        "def medida_recall(mat, l, k):\n",
        "    return mat[l,k]/float(np.sum(mat[l,:]))\n",
        "\n",
        "def medida_f1_especifica(mat, l, k):\n",
        "    prec = medida_precision(mat, l, k)\n",
        "    rec = medida_recall(mat, l, k)\n",
        "    if (prec+rec)==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2*prec*rec/(prec+rec)\n",
        "\n",
        "def medida_f1(mat):\n",
        "    totales = np.sum(mat,1)/float(np.sum(mat))\n",
        "    assign = np.sum([totales[l] * np.max([medida_f1_especifica(mat, l, k) \n",
        "                                          for k in np.arange(mat.shape[1])]) \n",
        "                     for l in np.arange(mat.shape[0])])\n",
        "    return assign\n",
        "\n",
        "mC = matriz_confusion(Dy,Dyp)\n",
        "\n",
        "print(mC)\n",
        "print('El valor del error cometido es = ', medida_error(mC))\n",
        "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC))\n",
        "print('El valor F1 es = ', medida_f1(mC))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEVFjPP7WM-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<h2>Implementaciones en librerías de Python</h2>\n",
        "\n",
        "La librería ScikitLearn ya implementa el algoritmo EM para mixturas de Gaussianas. "
      ]
    },
    {
      "metadata": {
        "id": "D72LrpjUWM-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Se inicializa el método con el número de clústeres (componentes) a buscar\n",
        "modelo = GaussianMixture(n_components = 4, max_iter = 200)\n",
        "# Se aprende el modelo\n",
        "modelo = modelo.fit(Dx)\n",
        "# Se predicen las asignaciones a clústeres\n",
        "Dyp_sk = modelo.predict(Dx)\n",
        "\n",
        "# Medimos el rendimiento del algoritmo de ScikitLearn\n",
        "mC_sk = matriz_confusion(Dy,Dyp_sk)\n",
        "\n",
        "print('Matriz de confusión:')\n",
        "print(mC_sk)\n",
        "print('El valor del error cometido es = ', medida_error(mC_sk))\n",
        "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC_sk))\n",
        "print('El valor F1 es = ', medida_f1(mC_sk))\n",
        "\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp_sk)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFCZCugnWM-O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "El algoritmo anterior, ejecutado a la primera, puede dar un resultado no óptimo: se está eligiendo una inicialización que encalla en un óptimo local. Si se ejecuta en varias ocaciones, eventualmente se obtendrá el resultado óptimo.\n",
        "\n",
        "Podríamos comparar el resultado de nuestro algoritmo y el de la implementación de ScikitLearn para observar si devuelven el mismo resultado:\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "f0mX8s7PWM-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Si comparamos el resultado de ambos algoritmos, el nuestro y el de ScikitLearn\n",
        "mC_comp = matriz_confusion(Dyp,Dyp_sk)\n",
        "\n",
        "print('Matriz de confusión:')\n",
        "print(mC_comp)\n",
        "print('El valor del error cometido es = ', medida_error(mC_comp))\n",
        "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC_comp))\n",
        "print('El valor F1 es = ', medida_f1(mC_comp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97xLqPAdWM-V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}